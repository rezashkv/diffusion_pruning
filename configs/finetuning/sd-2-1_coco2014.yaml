model:
  prediction_model:
    pretrained_model_name_or_path: stabilityai/stable-unet-2-1
    input_perturbation: 0.0
    revision: null
    resolution: 256
    use_ema: false
    noise_offset: 0.0
    prediction_type: v_prediction
    max_scheduler_steps: null
    unet_down_blocks:
      - CrossAttnDownBlock2DHalfGated
      - CrossAttnDownBlock2DHalfGated
      - CrossAttnDownBlock2DHalfGated
      - DownBlock2DHalfGated

    unet_mid_block: UNetMidBlock2DCrossAttnWidthGated

    unet_up_blocks:
      - UpBlock2DHalfGated
      - CrossAttnUpBlock2DHalfGated
      - CrossAttnUpBlock2DHalfGated
      - CrossAttnUpBlock2DHalfGated

    gated_ff: true
    ff_gate_width: 32

data:
  dataset_name: coco
  data_files: null
  dataset_config_name: null
  data_dir: "path/to/coco"

  max_train_samples: null
  max_validation_samples: 10
  year: 2014 # 2014 or 2017. 2014 is used in the paper.

  image_column: "image"
  caption_column: "caption"

  prompts: null
  max_generated_samples: 2

  dataloader:
    dataloader_num_workers: 0
    train_batch_size: 128
    validation_batch_size: 32
    image_generation_batch_size: 8
    center_crop: false
    random_flip: true

training:
  num_train_epochs: null
  max_train_steps: 30000
  hypernet_pretraining_steps: 500
  validation_steps: 1000
  image_logging_steps: 1000
  num_inference_steps: 50 # number of scheduler steps to run for image generation

  mixed_precision: null
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  local_rank: -1
  allow_tf32: false
  enable_xformers_memory_efficient_attention: false

  losses:
    diffusion_loss:
      snr_gamma: 5.0
      weight: 1.0

    resource_loss:
      type: log
      weight: 2.0
      pruning_target: 0.6

    contrastive_loss:
      arch_vector_temperature: 0.03
      prompt_embedding_temperature: 0.03
      weight: 100.0

    distillation_loss:
      weight: 0.2

    block_loss:
      weight: 0.2

    std_loss:
      weight: 0.1

    max_loss:
      weight: 0.1


  optim:
    prediction_model_learning_rate: 1e-5
    prediction_model_weight_decay: 0.00

    optimizer: "adamw"
    use_8bit_adam: false
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-08

    scale_lr: true
    lr_scheduler: "constant_with_warmup" # see pdm.utils.arg_utils for available options
    lr_warmup_steps: 250


  hf_hub:
    push_to_hub: false
    hub_token: null
    hub_model_id: null

  logging:
    logging_dir: "path/to/logs/"

    report_to: "wandb"
    tracker_project_name: "text2image-dynamic-pruning"
    wandb_log_dir: "path/to/wandb"

    checkpoints_total_limit: 1
    auto_checkpoint_step: false
    resume_from_checkpoint: latest # or null

